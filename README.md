# Tiny Transformer NLP

A minimal transformer-based language model written entirely from scratch in PyTorch â€” including a custom BPE tokenizer, training loop, and sampling-based text generation.

This project demonstrates the inner workings of a basic NLP model without relying on HuggingFace or any pretrained libraries.

---

## ðŸ§  Features

- âœ… Custom BPE tokenizer (no dependencies)
- âœ… Tiny transformer model (2 layers, multi-head attention)
- âœ… Pure PyTorch training and inference
- âœ… Text generation with top-k sampling
- âœ… GPU support
- âœ… Ready-to-use trained `.pt` model included

---

## ðŸš€ Usage

```bash
git clone https://github.com/onurbaydas1/tiny-transformer-nlp.git
cd tiny-transformer-nlp
pip install -r requirements.txt

# Train from scratch
python train.py

# Generate text
python generate.py
